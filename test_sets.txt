
Research:
-------------------

meaning/standardizing changes the images, why is that?

Use linear or log scale? (Prompt user?)

* R^2 is only relative to slope, not intercept
r^2 should never be negative, unless the regression line goes down (shouldnt be the case given both weights are positive, but seems to be)
r calculation when normalized wrong, r calculation non-normalized right

Cross-validation cost decreasing, but training cost increasing (hold-out), while R^2 increasing?

Better estimate despite worse cost? Plotting batches weird compared to actual loss

Why is the non-normalized cost so different compared to the normalized cost, despite giving similar results

should each model have a dataset assigned to it, i.e. if we want to train 3 separate models on 3 different datasets, or the same model on different datasets?
i.e. model.dataset

-------------------

Notes:
-------------------

* when k-folds cross-validation is used, no model is saved and it is used solely for evaluation, and as such you cannot make predictions or evaluate on a test set, the script returns right after printing the k models' metrics (as is, at least)


WARNING: Strings as inputs to the network are not properly implemented and haven't been properly tested. Also, when training models with a dependent variable that is a string, you will only be predict values with it once (after it finishes training), this is because a way to store and load dictionaries for string<->number conversions are yet to be implemented. The model architecture and weights can still be saved however. Just keep in mind that it won't work if you try to load it to make predictions.

datasets supported currently are .csv files, where columns are separated by commas ',' with no spaces, and a new-line to indicate a new row
or images in given folders containing a labels.txt file

* Example videos are sped-up

* Note: Features means are taken from the training set, before any normalization happens
* Features' minimum and maximum values, if normalizing, are taken after the mean and variance normalization, otherwise they're the training set's default values

NOTE: the "samples_feature_mean" in the model's json file refers to the mean of the features BEFORE normalization, I chose to leave it like this because it's necessary for normalization and the post-normalization mean can (and will) be calculated within the script


Note: Could not, with similar settings/network architecture, make a model that performs well predicting different car brands of the same color (in RGB). Though it seems to work well for predicting car colors, as well as digits from the MNIST dataset. I don't know the exact reason why it works for car colors, but not car brands of the same color. It's definitely a harder and more complex task to predict brands from the same colors, since it's a matter of differentiating shapes and edges, as opposed to colors which are very different while at the same time also being easily seen from the input data. This seems to be expected behavior, and likely solvable with the implementation of convolutional layers.

Note: 
*Do not normalize input if it's an image model and the image pixels are not in the range [0, 255]. I defined it so that whenever images are normalized, their range is [0, 255].
*Image models, when normalized are not mean normalized or standardized.

Note: If using the linked MNIST dataset, there is no need for normalization, as the features (pixels) are already between 0 and 1 and very small values. In fact, there is an issue when normalizing this dataset, which will result in some features'/pixels' mean values being `nan` (not a number). This occurs only for those pixels whose standard deviation is very close to 0 (i.e. 1.7792961104587026e-06, but works fine if it's a little further such as 0.0005059131940737591), or are exactly 0, because this results, respectively, in extremely big values, or an undefined value.

Note: Currently the script only works with images that all share the same dimensions (no need to be square)

Note: Explain why the derivative of the loss of the softmax/categorical cross entropy is defined differently than the others and less abstract

Note: Using stochastic gradient descent (when batch size is equal to training set size) will take much longer to converge than otherwise due to the frequency at which gradient descent is performed (every batch size training samples). Using the example dataset "dataset.csv", with independent variable "area_m2" and dependent variable "price_brl", learning rate of 0.5, batch size of 11293 (number of valid training samples), with 2 layers (input, output) with a linear activation function and mse as the loss function, it takes tens of thousands or hundreds of thousands of training steps for a decent model which is equivalent to the alternative 32 batch sized model, which takes only about 1000 steps at the same learning rate (the gradients exploded when increasing the learning rate above about ~0.65). The difference is lesser when the dataset is normalized, where a learning rate of 1000 and 100 steps is equivalent to the 32 batch sized model with a learning rate of 150 and 100 steps.

Note: Set the variable `plot_update_every_n_iterations` to a value greater than `batch_size * steps` in order to not plot anything, and equal to in order to plot only after training finishes

Note: xavier initialization for sigmoid, softmax, linear
        he for relu

Note: Dataset loader skips first row and expects each variable to be a different column

Note: Currently there's no way to save any of the models when training with k-folds cross-validation, so if you believe that you got good enough metrics and wanted to save any of them, you will need to re-run the script without using k-folds, which also means the resulting model will be different to some extent.

Note: If batch size is set to be a value that's greater than the number of samples in the training set(s), the value is clipped to be the number of training samples. (So you can also input a batch size that you know is bigger than the number of samples in your training set even if you don't know the exact number, in order to force batch gradient descent)

Note: (K-Folds) For now, if the number of training samples is not exactly divisible by the number of folds, the spare sample will be ignored

Note: Image labels cannot have duplicates since they're identified by filenames (within current directory).

Note: 4 -> 4 framingham metrics results in Precision: 1 | Accuracy: 0.99375 | Recall: 0.0 | F1-score: 0.0. This is likely because there are very few cases for which there is `prevalentStroke`, so the model gets addicted to always predicting 0. How can I avoid this?

Note: The script is not optimized, very slow, memory heavy (depending on the dataset), sometimes the script may crash due to this. One example is that in my computer I have 32 GB RAM, and running the script with the MNIST dataset (60000 samples 28*28 black and white images), no graphing and using a test set that's 50% of the total samples (so 30000 training samples and 30000 testing samples), with a learning rate of 0.003, batch size of 1, and 5 training steps, my computer crashed in between the 4th and 5th steps. 

Note: By design choice, normalization is within the range [-1, 1], not [0, 1] (excluding mean normalization and standardization).

Note: The global variables (such as `training_features_mean`, `crossvalidation_features_mean`, `test_features_mean`) that keep track of means, variances, and stds, are taken after normalization (if normalized), mean normalization and standardizing, while the alternative class members such as `model.sample_features_mean` of the `model` variable  which is an instance of the class `Model`, are stored after normalizing, but before mean normalizing and standardizing. They're separate as the latter is used for further metrics evaluations, during and after training, while also being used to revert the normalization or normalize non-normalized values, while the former also aids in reverting the normalization and the normalization of non-normalized values.

Note: The script expects all images to have pixel colors in the range [0, 255]. You can change that in the function `mean_n_variance_normalize`, by changing the feature min maxes from 0 and 255 to 0 and 1, respectively.

-------------------

Todo:
-------------------

make sure string inputs are working properly
line 373 dataset.py, turning whole sentences into random numbers makes them lose any predictive properties. turn sentences into a list of words, and assign each word a unique value that will be used for every instance of that word
i.e. 
instead of 
"i am having headaches right now" -> 3451
"happy" -> 4123
"i am not happy right now" -> 2
have
"i am having headaches right now" -> ["i", "am", "having", "headaches", "right", "now"] -> [4, 1, 23, 44, 3, 130]
"happy" -> ["happy"] -> [88]

variable nodes, one input node for word? each word in the vocabulary has an input node that gets activated/deactivated depending on its presence in the sentence? (ignores word ordering)


ask user whether to store micro, macro, model info

let user give learning rate, as well as regularization, and what type(s) (l1 and/or l2) if the user so chooses

allow hyperparameters tweaking during cross validation i.e. regularization, option to add/remove features (feature selection), hidden layers (if more than 1), hidden layer nodes, and output nodes, for cross validation models


option "use default settings?" to just run with settings already set in the script (i.e. steps, learning rate, dataset, training set percentage, use test set, no k-folds, etc...)
separate validation/test at the end in a function

. gradient checking
(Approximate derivatives instead of calculating exact derivative function, with custom epsilons) (https://www.coursera.org/learn/machine-learning-course/lecture/Y3s6r/gradient-checking) 
Numerical estimation of the partial derivatives? Gradient descent?

. gradient clipping
. dropout
(doesn't work with gradient checking)
layer(0, 1, 5, dropout=true, dropout_rate=0.25, regularize=true, regularization_rate=0.1)
keep probability for testing dropout (1 - dropout_rate)?
* layer/batch normalization

swap variables names and standardize them

option to evaluate loaded model on test set without having to train it (imagine someone loaded a model and just wanted to check its performance metrics on a given dataset)

implement convolutional layer


add mid-training weight checkpoints

let user save/predict once finished training any (or all) k-folds model(s)

implement a vocabulary/dictionary?  for conversion from strings to numbers when only predicting and don't know the dataset?

nan_values -> finished training -> save_file("model_k_dictionary") <- contains "str": "int" pairs within a dictionary (keeps track of how to convert strings into numbers for prediction when not training the model from scratch/loading a dataset)
keep track of whether model was normalized?

how to make matplotlib faster, what does cache_frame_data= do

divide by `n - 1` variance to conform with bias?

Make R (correlation coefficient) calculation separate from the "measure dataset", as it's only measured once and doesn't change during training (unlike R^2)

update plot instead of creating new one

implement other optimizers/adaptive learning rates such as adagrad/adam/rmsprop, etc...

implement (optional) batch normalization: https://www.coursera.org/learn/deep-neural-network/lecture/4ptp2/normalizing-activations-in-a-network

separate scripts like
dataset.py
train.py
utils.py
plot/graph.py
model.py

decouple some functions in order to allow for having multiple models and being able to use them independently, without relying on global variables, as well as being able to do everything without having to rerun the script


add option for use to choose which features to normalize, instead of automatically normalizing all features

perform `measure_model_on_dataset` arbitrarily instead of for every batch, i.e. every step or every n samples. user picked? (cross-validation too slow because this is done too often for low sized batches)

arbitrarily smooth graph? exponentially moving average?

implement svm
implement random forests
implement decision trees
implement stratified sampling (for classification problems only?)
implement k nearest neighbor

- Use jacobian matrices, speed up calculations with SIMD instructions and vectorize everything possible
move image loading to dataset.py


- Leverage GPU (Nvidia) CUDA cores for processing speed

- Implement activation functions for each individual node instead of the entire layer if the user so wishes

- Browser UI, better graphing/user experience, visualize how the graphs are affected by the activation functions and weights at each layer in a user interface such as the browser?

- Estimate time remaining for training to finish

- Checkpoitns to rollback at points in which the validation cost and training cost were at their lowest

- separate the bias from the dataset/normal weights? or leave it as a regular weight/variable?

- prompt user whether or not to shuffle dataset

- Possible future experiments: Convolutional layers, sentiment analysis, natural language processing, implementing transformers, generative models

let user pick batch gradient descent by selecting the batch size to be the entire dataset (automatically figure size for k-folds, when testing, or normal), or manually input

plot_update_every_n_batches -> let user choose whether to give a manual custom number of batches to update the plot, or leave by default in order to update plot for every training step (i.e. plot_update_every_n_batches = n_training_samples / batch_size)

- Option to convert image to grayscale

-------------------

age
totChol
sysBP
glucose


prevalentHyp
prevalentStroke
currentSmoker
diabetes


-------

test/make sure you can test images model that are not within the same folder

test linear regression with multiple features
test k-folds
test regularization
make videos
write readme
post
watch remaining simple videos (i.e. convolutional video, read texts)
why do the k-folds models behave so distinctively despite sharing the same learning rate

Correct
Intercept: 255047.28429923143
B1: 3617.16522849 (area -> price)


vids:
linear regression: area_m2 -> price_brl
normalizing, not normalizing
filtering dataset linear regression
cross-validation: k-folds, hold-out
test set
plotting, not plotting
distance from reference
logistic regression: sysBP -> prevalentHyp
multiclass: mnist (28*28 black white images) -> [0...9]
multiclass: car color -> [0, 1, 2]
multilabel: age, totChol, sysBP, glucose -> prevalentHyp, prevalentStroke, currentSmoker, diabetes


run each example and store working parameters and json model
videos after
readme after
easy fixes after
watch videos after

Models/parameters
________________
houses.csv
normalized
(linear regression) area_m2 -> price_brl
1 -> 1 (linear) / loss_mse
lr = 0.1
steps = 100
batch_size = 11293
plot_update_every_n_batches = 1
* hold-out (show console measures)
--------------------------

framingham.csv
normalized
(logistic regression) sysBP -> prevalentHyp
1 -> 5 (relu) -> 1 (sigmoid) / loss binary cross-entropy
lr = 1
steps = 50
batch_size = 300
plot_update_every_n_batches = 12
* hold-out

normalized
(multilabel classification) age, totChol, sysBP, glucose -> prevalentHyp, prevalentStroke, currentSmoker, diabetes
4 -> 50 (relu) -> 4 (sigmoid) / loss binary cross-entropy
lr = 1
steps = 230
batch_size = 300
plot_update_every_n_batches = 12
* do 3-folds graphing
--------------------------

--------------------------
mnist

(multiclass classification) 28x28 black and white images -> [0..9]
28x28 -> 50 (relu) -> 10 (softmax) / loss categorical cross-entropy
normalized
lr = 0.08
steps = 5
batch_size = 32
plot_update_every_n_batches = 0
0.5 test set
(draw on paint numbers 0-9 and show prediction "test.png")

--------------------------

--------------------------
car colors

(multiclass classification) 106x28 rgb images -> [0..2] (black, red, blue)
106x80x3 -> 50 (relu) -> 3 (softmax) / loss
normalized
lr = 0
steps = 0
batch_size = 0
plot_update_every_n_batches = 0

* test set (separate folders?)

--------------------------


Videos
----------------
Show dataset at beginning
In the case of mnist, cars, show dataset and labeling
Show console metrics if present
Show saved model if saved
Load model to test after save

Cut icons, speed up slow sections/training
Separate video for dataset manipulation
Separate video for resizing
Separate video for labeling/resizing images

----------------

README
----------------
Explain utils
----------------


update json models
plot black and white, colored images, normalized and non-normalized to check if weird

standardize and mean normalize, find way to properly plot regardless of normalized or not
test separate folders
redo other json models
test saving, loading all models
delete extra/duplicate/renamed things from folder and repo


NORMALIZING MAKES DARKER/LOSES DETAILS (AT LEAST WHEN PLOTTED)